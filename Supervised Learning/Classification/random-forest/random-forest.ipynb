{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "5a0261d5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Classifier from Scratch\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, classification_report, accuracy_score\n",
    "from collections import Counter\n",
    "\n",
    "# Set random seed for reproducible results across multiple runs\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "cf438bbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Node Class: Represents a single node in the decision tree\n",
    "# This class stores all the information needed for each node in our tree structure\n",
    "\n",
    "class Node():\n",
    "    def __init__(self, feature_index=None, threshold=None, left=None, right=None, info_gain=None, value=None):\n",
    "        # feature_index: Which feature (column) to split on at this node\n",
    "        self.feature_index = feature_index\n",
    "        \n",
    "        # threshold: The value to compare the feature against for splitting\n",
    "        self.threshold = threshold\n",
    "        \n",
    "        # left: Reference to the left child node (samples <= threshold)\n",
    "        self.left = left\n",
    "        \n",
    "        # right: Reference to the right child node (samples > threshold)\n",
    "        self.right = right\n",
    "        \n",
    "        # info_gain: How much information gain this split provides\n",
    "        self.info_gain = info_gain\n",
    "        \n",
    "        # value: The predicted class for leaf nodes (None for internal nodes)\n",
    "        self.value = value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "66818922",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Decision Tree Class: Modified for Random Forest with Feature Randomness\n",
    "# This is a single decision tree that will be used as a component in our Random Forest\n",
    "\n",
    "class DecisionTreeForRF():\n",
    "    def __init__(self, min_sample_split=2, max_depth=10, criterion=\"entropy\", max_features=None):\n",
    "        # root: The top node of our decision tree\n",
    "        self.root = None\n",
    "        \n",
    "        # min_sample_split: Minimum samples required to split a node (prevents overfitting)\n",
    "        self.min_sample_split = min_sample_split\n",
    "        \n",
    "        # max_depth: Maximum depth of the tree (prevents overfitting)\n",
    "        self.max_depth = max_depth\n",
    "        \n",
    "        # criterion: How to measure the quality of a split (\"entropy\" or \"gini\")\n",
    "        self.criterion = criterion\n",
    "        \n",
    "        # max_features: Number of features to consider at each split (key for Random Forest)\n",
    "        self.max_features = max_features\n",
    "    \n",
    "    def fit(self, X, y):\n",
    "        \"\"\"Train the decision tree on the given data\"\"\"\n",
    "        # Store the total number of features in the dataset\n",
    "        self.n_features = X.shape[1]\n",
    "        \n",
    "        # If max_features not specified, use square root of total features (common for classification)\n",
    "        if self.max_features is None:\n",
    "            self.max_features = int(np.sqrt(self.n_features))\n",
    "\n",
    "        # Combine features and labels into one dataset for easier handling\n",
    "        dataset = np.concatenate((X, y.reshape(-1,1)), axis=1)\n",
    "        \n",
    "        # Start building the tree from the root\n",
    "        self.root = self.build_tree(dataset)\n",
    "\n",
    "    def build_tree(self, dataset: np.ndarray, curr_depth=0):\n",
    "        \"\"\"Recursively build the decision tree using the dataset\"\"\"\n",
    "        # Separate features (X) and labels (y) from the combined dataset\n",
    "        X, y = dataset[:, :-1], dataset[:, -1]\n",
    "        num_samples, num_features = X.shape\n",
    "\n",
    "        # Check stopping criteria for tree growth\n",
    "        if num_samples >= self.min_sample_split and curr_depth <= self.max_depth:\n",
    "            # Try to find the best possible split for this node\n",
    "            best_split = self.get_best_split(dataset, num_samples, num_features)\n",
    "\n",
    "            # If we found a good split (positive information gain)\n",
    "            if best_split.get(\"info_gain\", 0) > 0:\n",
    "                # Recursively build left subtree with samples that go left\n",
    "                left_subtree = self.build_tree(best_split[\"dataset_left\"], curr_depth+1)\n",
    "                \n",
    "                # Recursively build right subtree with samples that go right\n",
    "                right_subtree = self.build_tree(best_split[\"dataset_right\"], curr_depth+1)\n",
    "\n",
    "                # Create internal node with the best split information\n",
    "                return Node(best_split[\"feature_index\"], best_split[\"threshold\"], \n",
    "                            left_subtree, right_subtree, best_split[\"info_gain\"])\n",
    "            \n",
    "        # If stopping criteria met or no good split found, create a leaf node\n",
    "        leaf_node = self.leaf_node(y)\n",
    "        return Node(value=leaf_node)\n",
    "    \n",
    "    def get_best_split(self, dataset, num_samples, num_features):\n",
    "        \"\"\"Find the best feature and threshold to split on using RANDOM FEATURES (key RF modification)\"\"\"\n",
    "        best_split = {}\n",
    "        max_info_gain = -float(\"inf\")\n",
    "\n",
    "        # RANDOM FOREST KEY FEATURE: Randomly select subset of features to consider\n",
    "        # This creates diversity between trees in the forest\n",
    "        available_features = list(range(num_features))\n",
    "        selected_features = np.random.choice(available_features, \n",
    "                                           size=min(self.max_features, num_features), \n",
    "                                           replace=False)\n",
    "\n",
    "        # Try only the randomly selected features (not all features)\n",
    "        for feature_index in selected_features:\n",
    "            feature_values = dataset[:, feature_index]\n",
    "            possible_thresholds = np.unique(feature_values)\n",
    "            \n",
    "            # Try different threshold values for this feature\n",
    "            for i in range(1, len(possible_thresholds)):\n",
    "                # Calculate threshold as midpoint between consecutive unique values\n",
    "                threshold = (possible_thresholds[i] + possible_thresholds[i-1]) / 2\n",
    "                \n",
    "                # Split the dataset based on this threshold\n",
    "                dataset_left, dataset_right = self.split(feature_index, dataset, threshold)\n",
    "\n",
    "                # Only consider splits that create non-empty subsets\n",
    "                if len(dataset_left) > 0 and len(dataset_right) > 0:\n",
    "                    y, y_left, y_right = dataset[:, -1], dataset_left[:, -1], dataset_right[:, -1]\n",
    "                    \n",
    "                    # Calculate how much information gain this split provides\n",
    "                    curr_info_gain = self.information_gain(y, y_left, y_right, self.criterion)\n",
    "\n",
    "                    # Keep track of the best split found so far\n",
    "                    if curr_info_gain > max_info_gain:\n",
    "                        best_split[\"feature_index\"] = feature_index\n",
    "                        best_split[\"threshold\"] = threshold\n",
    "                        best_split[\"dataset_left\"] = dataset_left\n",
    "                        best_split[\"dataset_right\"] = dataset_right\n",
    "                        best_split[\"info_gain\"] = curr_info_gain\n",
    "                        max_info_gain = curr_info_gain\n",
    "        \n",
    "        return best_split\n",
    "\n",
    "    def split(self, feature_index, dataset, threshold):\n",
    "        \"\"\"Split the dataset based on a feature and threshold\"\"\"\n",
    "        # Left split: samples where feature value <= threshold\n",
    "        dataset_left = np.array([row for row in dataset if row[feature_index] <= threshold])\n",
    "        \n",
    "        # Right split: samples where feature value > threshold\n",
    "        dataset_right = np.array([row for row in dataset if row[feature_index] > threshold])\n",
    "        \n",
    "        return dataset_left, dataset_right\n",
    "\n",
    "    def information_gain(self, parent, left_child, right_child, criterion):\n",
    "        \"\"\"Calculate the information gain from a split\"\"\"\n",
    "        # Calculate weights based on the size of each child\n",
    "        weight_left = len(left_child) / len(parent)\n",
    "        weight_right = len(right_child) / len(parent)\n",
    "\n",
    "        # Calculate information gain using the specified criterion\n",
    "        if criterion == \"entropy\":\n",
    "            # Information Gain = Parent Entropy - Weighted Average of Child Entropies\n",
    "            info_gain_val = self.entropy(parent) - (weight_left * self.entropy(left_child) + weight_right * self.entropy(right_child))\n",
    "        elif criterion == \"gini\":\n",
    "            # Information Gain = Parent Gini - Weighted Average of Child Gini\n",
    "            info_gain_val = self.gini(parent) - (weight_left * self.gini(left_child) + weight_right * self.gini(right_child))\n",
    "        return info_gain_val\n",
    "        \n",
    "    def entropy(self, y):\n",
    "        \"\"\"Calculate entropy (measure of disorder/impurity) of a dataset\"\"\"\n",
    "        entropy_val = 0\n",
    "        class_labels = np.unique(y)\n",
    "        \n",
    "        # Entropy = -Σ(p_i * log2(p_i)) where p_i is probability of class i\n",
    "        for _class in class_labels:\n",
    "            prob_class = len(y[y == _class]) / len(y)\n",
    "            entropy_val += -prob_class * np.log2(prob_class)\n",
    "        return entropy_val\n",
    "        \n",
    "    def gini(self, y):\n",
    "        \"\"\"Calculate Gini impurity (alternative to entropy) of a dataset\"\"\"\n",
    "        gini_val = 1\n",
    "        class_labels = np.unique(y)\n",
    "        \n",
    "        # Gini = 1 - Σ(p_i^2) where p_i is probability of class i\n",
    "        for _class in class_labels:\n",
    "            prob_class = len(y[y==_class]) / len(y)\n",
    "            gini_val -= prob_class ** 2\n",
    "        return gini_val\n",
    "\n",
    "    def leaf_node(self, y):\n",
    "        \"\"\"Create a leaf node by finding the most common class in the data\"\"\"\n",
    "        unique_classes, counts = np.unique(y, return_counts=True)\n",
    "        # Return the class with the highest count (majority vote)\n",
    "        return unique_classes[np.argmax(counts)]\n",
    "    \n",
    "    def make_predictions(self, X, tree):\n",
    "        \"\"\"Recursively traverse the tree to make a prediction for a single sample\"\"\"\n",
    "        # If we reached a leaf node, return its prediction\n",
    "        if tree.value is not None:\n",
    "            return tree.value\n",
    "        \n",
    "        # Get the feature value for the current node's feature\n",
    "        feature_val = X[tree.feature_index]\n",
    "        \n",
    "        # Decide which direction to go based on the threshold\n",
    "        if feature_val <= tree.threshold:\n",
    "            return self.make_predictions(X, tree.left)  # Go left\n",
    "        else:\n",
    "            return self.make_predictions(X, tree.right)  # Go right\n",
    "        \n",
    "    def predict(self, X):\n",
    "        \"\"\"Make predictions for all samples in X\"\"\"\n",
    "        predictions = []\n",
    "        # Make prediction for each sample individually\n",
    "        for data_point in X:\n",
    "            prediction = self.make_predictions(data_point, self.root)\n",
    "            predictions.append(prediction)\n",
    "        return np.array(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "d4ada208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bootstrap Sampling Function: Creates diverse training sets for each tree\n",
    "# This is a key component of Random Forest that introduces randomness in the data\n",
    "\n",
    "def bootstrap_sample(X, y):\n",
    "    \"\"\"\n",
    "    Create a bootstrap sample (random sample with replacement) from the training data.\n",
    "    \n",
    "    Bootstrap sampling allows some samples to appear multiple times and others to be left out,\n",
    "    creating different training sets for each tree in the forest.\n",
    "    \n",
    "    Args:\n",
    "        X: Feature matrix\n",
    "        y: Target labels\n",
    "    \n",
    "    Returns:\n",
    "        X_bootstrap, y_bootstrap: Bootstrap sample of the same size as original data\n",
    "    \"\"\"\n",
    "    # Get the number of samples in the original dataset\n",
    "    n_samples = X.shape[0]\n",
    "    \n",
    "    # Randomly select indices WITH replacement (key: replace=True)\n",
    "    # This means some samples may appear multiple times, others may not appear at all\n",
    "    indices = np.random.choice(n_samples, size=n_samples, replace=True)\n",
    "    \n",
    "    # Return the bootstrap sample using the selected indices\n",
    "    return X[indices], y[indices]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "a4d8d52f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Random Forest Class: Ensemble of Decision Trees with Voting\n",
    "# Combines multiple decision trees to create a more robust and accurate model\n",
    "\n",
    "class RandomForest():\n",
    "    def __init__(self, n_estimators=100, max_depth=10, min_sample_split=2,\n",
    "                 max_features=None, criterion=\"entropy\", random_state=None):\n",
    "        \"\"\"\n",
    "        Initialize Random Forest Classifier\n",
    "        \n",
    "        Args:\n",
    "            n_estimators: Number of decision trees in the forest\n",
    "            max_depth: Maximum depth of each individual tree\n",
    "            min_sample_split: Minimum samples required to split a node\n",
    "            max_features: Number of features to consider at each split\n",
    "            criterion: Splitting criterion (\"entropy\" or \"gini\")\n",
    "            random_state: Random seed for reproducible results\n",
    "        \"\"\"\n",
    "        self.n_estimators = n_estimators\n",
    "        self.max_depth = max_depth\n",
    "        self.min_sample_split = min_sample_split\n",
    "        self.max_features = max_features\n",
    "        self.criterion = criterion\n",
    "        self.random_state = random_state\n",
    "\n",
    "        # List to store all the trained decision trees\n",
    "        self.trees = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        \"\"\"\n",
    "        Train the Random Forest by creating and training multiple decision trees\n",
    "        \n",
    "        Each tree is trained on a different bootstrap sample of the data,\n",
    "        ensuring diversity in the forest.\n",
    "        \"\"\"\n",
    "        # Set random seed for reproducible results\n",
    "        if self.random_state:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        # Reset trees list (important for retraining)\n",
    "        self.trees = []\n",
    "\n",
    "        # Train each tree in the forest\n",
    "        for i in range(self.n_estimators):\n",
    "            # Step 1: Create a bootstrap sample (random sample with replacement)\n",
    "            X_bootstrap, y_bootstrap = bootstrap_sample(X, y)\n",
    "\n",
    "            # Step 2: Create a new decision tree with the specified parameters\n",
    "            tree = DecisionTreeForRF(min_sample_split=self.min_sample_split,\n",
    "                                     max_depth=self.max_depth,\n",
    "                                     criterion=self.criterion,\n",
    "                                     max_features=self.max_features\n",
    "                                     )\n",
    "            \n",
    "            # Step 3: Train the tree on the bootstrap sample\n",
    "            tree.fit(X_bootstrap, y_bootstrap)\n",
    "\n",
    "            # Step 4: Add the trained tree to our forest\n",
    "            self.trees.append(tree)\n",
    "\n",
    "            # Progress indicator (print every 20 trees or the first tree)\n",
    "            if (i + 1) % 20 == 0 or i == 0:\n",
    "                print(f\"Trained {i + 1}/{self.n_estimators} trees\")\n",
    "        \n",
    "        return self  # Return self for method chaining\n",
    "\n",
    "    def predict(self, X):\n",
    "        \"\"\"\n",
    "        Make predictions using majority voting from all trees in the forest\n",
    "        \n",
    "        Each tree votes for a class, and the final prediction is the class\n",
    "        that receives the most votes.\n",
    "        \"\"\"\n",
    "        # Collect predictions from all trees\n",
    "        tree_predictions = []\n",
    "\n",
    "        # Get prediction from each tree in the forest\n",
    "        for tree in self.trees:\n",
    "            pred = tree.predict(X)\n",
    "            tree_predictions.append(pred)\n",
    "\n",
    "        # Convert to numpy array for easier manipulation\n",
    "        # Shape: (n_trees, n_samples)\n",
    "        tree_predictions = np.array(tree_predictions)\n",
    "\n",
    "        # Perform majority voting for each sample\n",
    "        final_predictions = []\n",
    "        for i in range(X.shape[0]):  # For each test sample\n",
    "            # Get all tree predictions for this sample\n",
    "            sample_predictions = tree_predictions[:, i]\n",
    "\n",
    "            # Count votes for each class\n",
    "            vote_counts = Counter(sample_predictions)\n",
    "            \n",
    "            # Get the class with the most votes (majority vote)\n",
    "            majority_vote = vote_counts.most_common(1)[0][0]\n",
    "            final_predictions.append(majority_vote)\n",
    "\n",
    "        return np.array(final_predictions)\n",
    "    \n",
    "    def predict_proba(self, X):\n",
    "        \"\"\"\n",
    "        Get class probabilities by calculating the proportion of votes for each class\n",
    "        \n",
    "        Instead of just returning the majority vote, this returns the probability\n",
    "        of each class based on the proportion of trees that voted for it.\n",
    "        \"\"\"\n",
    "        # Collect predictions from all trees\n",
    "        tree_predictions = []\n",
    "\n",
    "        # Get prediction from each tree in the forest\n",
    "        for tree in self.trees:\n",
    "            pred = tree.predict(X)\n",
    "            tree_predictions.append(pred)\n",
    "\n",
    "        # Convert to numpy array: Shape (n_trees, n_samples)\n",
    "        tree_predictions = np.array(tree_predictions)\n",
    "\n",
    "        # Calculate probabilities for each sample\n",
    "        probabilities = []\n",
    "        for i in range(X.shape[0]):  # For each test sample\n",
    "            # Get all tree predictions for this sample\n",
    "            sample_predictions = tree_predictions[:, i]\n",
    "\n",
    "            # Count votes for each class\n",
    "            vote_counts = Counter(sample_predictions)\n",
    "            total_votes = len(sample_predictions)\n",
    "\n",
    "            # Calculate probability for each unique class\n",
    "            prob_list = []\n",
    "            for cls in np.unique(tree_predictions):\n",
    "                prob = vote_counts[cls] / total_votes\n",
    "                prob_list.append(prob)\n",
    "            \n",
    "            probabilities.append(prob_list)\n",
    "        return np.array(probabilities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "743bbdb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>842302</td>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>...</td>\n",
       "      <td>25.38</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>842517</td>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>...</td>\n",
       "      <td>24.99</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>84300903</td>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>...</td>\n",
       "      <td>23.57</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>84348301</td>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>...</td>\n",
       "      <td>14.91</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>84358402</td>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>...</td>\n",
       "      <td>22.54</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         id diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "0    842302         M        17.99         10.38          122.80     1001.0   \n",
       "1    842517         M        20.57         17.77          132.90     1326.0   \n",
       "2  84300903         M        19.69         21.25          130.00     1203.0   \n",
       "3  84348301         M        11.42         20.38           77.58      386.1   \n",
       "4  84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "   smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "0          0.11840           0.27760          0.3001              0.14710   \n",
       "1          0.08474           0.07864          0.0869              0.07017   \n",
       "2          0.10960           0.15990          0.1974              0.12790   \n",
       "3          0.14250           0.28390          0.2414              0.10520   \n",
       "4          0.10030           0.13280          0.1980              0.10430   \n",
       "\n",
       "   ...  radius_worst  texture_worst  perimeter_worst  area_worst  \\\n",
       "0  ...         25.38          17.33           184.60      2019.0   \n",
       "1  ...         24.99          23.41           158.80      1956.0   \n",
       "2  ...         23.57          25.53           152.50      1709.0   \n",
       "3  ...         14.91          26.50            98.87       567.7   \n",
       "4  ...         22.54          16.67           152.20      1575.0   \n",
       "\n",
       "   smoothness_worst  compactness_worst  concavity_worst  concave points_worst  \\\n",
       "0            0.1622             0.6656           0.7119                0.2654   \n",
       "1            0.1238             0.1866           0.2416                0.1860   \n",
       "2            0.1444             0.4245           0.4504                0.2430   \n",
       "3            0.2098             0.8663           0.6869                0.2575   \n",
       "4            0.1374             0.2050           0.4000                0.1625   \n",
       "\n",
       "   symmetry_worst  fractal_dimension_worst  \n",
       "0          0.4601                  0.11890  \n",
       "1          0.2750                  0.08902  \n",
       "2          0.3613                  0.08758  \n",
       "3          0.6638                  0.17300  \n",
       "4          0.2364                  0.07678  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the Breast Cancer Dataset\n",
    "# This dataset contains features of cell nuclei from breast cancer biopsies\n",
    "# Goal: Predict whether a tumor is Malignant (M) or Benign (B)\n",
    "\n",
    "# Load the CSV file containing the breast cancer data\n",
    "data = pd.read_csv(\"Datasets\\data.csv\")\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "09ec4803",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set shapes: X_train=(455, 30), y_train=(455,)\n",
      "Testing set shapes: X_test=(114, 30), y_test=(114,)\n",
      "Total features: 30\n",
      "Total training samples: 455\n",
      "Total testing samples: 114\n"
     ]
    }
   ],
   "source": [
    "# Data Preprocessing: Prepare features and target variable\n",
    "# Separate the features from the target variable and encode the labels\n",
    "\n",
    "# Features (X): All columns except 'diagnosis' and 'id'\n",
    "# Remove 'diagnosis' (target) and 'id' (irrelevant identifier) columns\n",
    "X = data.drop(columns=['diagnosis', 'id']).values  \n",
    "\n",
    "# Target variable (y): Encode diagnosis as numerical values\n",
    "# Convert M (Malignant) to 1 and B (Benign) to 0 for binary classification\n",
    "# Reshape to column vector for compatibility with our algorithms\n",
    "y = data['diagnosis'].map({'M': 1, 'B': 0}).values\n",
    "\n",
    "# Split the dataset into training and testing sets\n",
    "# 80% for training, 20% for testing, with stratification to maintain class balance\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Display the shapes to verify the split worked correctly\n",
    "print(f\"Training set shapes: X_train={X_train.shape}, y_train={y_train.shape}\")\n",
    "print(f\"Testing set shapes: X_test={X_test.shape}, y_test={y_test.shape}\")\n",
    "print(f\"Total features: {X_train.shape[1]}\")\n",
    "print(f\"Total training samples: {X_train.shape[0]}\")\n",
    "print(f\"Total testing samples: {X_test.shape[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "6edeaf24",
   "metadata": {},
   "outputs": [
    {
     "ename": "IndexError",
     "evalue": "too many indices for array: array is 1-dimensional, but 2 were indexed",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 6\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m# Data Visualization: Plot the distribution of Benign vs Malignant tumors\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m# Visualize the first two features (radius_mean and texture_mean) to understand data distribution\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Extract data points for visualization from the training set\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Get all benign (B=0) tumor data points from training set\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m benign_points \u001b[38;5;241m=\u001b[39m X_train[\u001b[43my_train\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m]\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Get all malignant (M=1) tumor data points from training set  \u001b[39;00m\n\u001b[0;32m      9\u001b[0m malignant_points \u001b[38;5;241m=\u001b[39m X_train[y_train[:, \u001b[38;5;241m0\u001b[39m] \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m]\n",
      "\u001b[1;31mIndexError\u001b[0m: too many indices for array: array is 1-dimensional, but 2 were indexed"
     ]
    }
   ],
   "source": [
    "# Data Visualization: Plot the distribution of Benign vs Malignant tumors\n",
    "# Visualize the first two features (radius_mean and texture_mean) to understand data distribution\n",
    "\n",
    "# Extract data points for visualization from the training set\n",
    "# Get all benign (B=0) tumor data points from training set\n",
    "benign_points = X_train[y_train[:, 0] == 0]\n",
    "\n",
    "# Get all malignant (M=1) tumor data points from training set  \n",
    "malignant_points = X_train[y_train[:, 0] == 1]\n",
    "\n",
    "# Create a scatter plot to visualize the data distribution\n",
    "plt.figure(figsize=(10, 8))\n",
    "\n",
    "# Plot benign tumors in green (typically smaller radius and different texture)\n",
    "plt.scatter(benign_points[:, 0], benign_points[:, 1], \n",
    "           color=\"tab:green\", label=\"Benign\", alpha=0.7, s=50)\n",
    "\n",
    "# Plot malignant tumors in red (typically larger radius and different texture)\n",
    "plt.scatter(malignant_points[:, 0], malignant_points[:, 1], \n",
    "           color=\"tab:red\", label=\"Malignant\", alpha=0.7, s=50)\n",
    "\n",
    "# Add axis labels (using first two features from the dataset)\n",
    "plt.xlabel(\"radius_mean (Feature 0)\", fontsize=12)\n",
    "plt.ylabel(\"texture_mean (Feature 1)\", fontsize=12)\n",
    "plt.title(\"Breast Cancer Data Distribution: Benign vs Malignant\", fontsize=14)\n",
    "plt.legend(fontsize=12)\n",
    "plt.grid(True, alpha=0.3)\n",
    "\n",
    "# Add some statistics to the plot\n",
    "print(f\"Training set distribution:\")\n",
    "print(f\"Benign samples: {len(benign_points)} ({len(benign_points)/len(y_train)*100:.1f}%)\")\n",
    "print(f\"Malignant samples: {len(malignant_points)} ({len(malignant_points)/len(y_train)*100:.1f}%)\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "0487a245",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "TRAINING CUSTOM RANDOM FOREST\n",
      "==================================================\n",
      "Trained 1/100 trees\n",
      "Trained 20/100 trees\n",
      "Trained 40/100 trees\n",
      "Trained 60/100 trees\n",
      "Trained 80/100 trees\n",
      "Trained 100/100 trees\n",
      "==================================================\n",
      "TRAINING COMPLETED - PREDICTIONS MADE\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "# Train Our Custom Random Forest Model\n",
    "# Create and train a Random Forest with specified hyperparameters\n",
    "\n",
    "# Initialize Random Forest with carefully chosen parameters\n",
    "rf = RandomForest(\n",
    "    n_estimators=100,       # Use 100 decision trees in the forest\n",
    "    max_depth=10,           # Limit tree depth to prevent overfitting\n",
    "    min_sample_split=2,     # Minimum samples required to split a node\n",
    "    criterion=\"gini\",       # Use entropy for measuring split quality\n",
    "    random_state=34         # Set seed for reproducible results\n",
    ")\n",
    "\n",
    "# Train the model and immediately make predictions (method chaining)\n",
    "# This demonstrates the fit() method returning self for chaining\n",
    "print(\"=\"*50)\n",
    "print(\"TRAINING CUSTOM RANDOM FOREST\")\n",
    "print(\"=\"*50)\n",
    "\n",
    "y_pred = rf.fit(X_train, y_train).predict(X_test)\n",
    "\n",
    "print(\"=\"*50)\n",
    "print(\"TRAINING COMPLETED - PREDICTIONS MADE\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9702cf76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "CUSTOM RANDOM FOREST PERFORMANCE METRICS\n",
      "==================================================\n",
      "Confusion Matrix:\n",
      "[[70  1]\n",
      " [ 3 40]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        71\n",
      "           1       0.98      0.93      0.95        43\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "Accuracy: 96.49%\n",
      "==================================================\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"CUSTOM RANDOM FOREST PERFORMANCE METRICS\")\n",
    "print(\"=\"*50)\n",
    "# Evaluate the model's performance using various metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, y_pred)*100:.2f}%\")\n",
    "print(\"=\"*50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6705930a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING SCIKIT-LEARN RANDOM FOREST FOR COMPARISON\n",
      "============================================================\n",
      "scikit-learn Random Forest Predictions Completed\n"
     ]
    }
   ],
   "source": [
    "# Compare with Scikit-Learn's Random Forest Implementation\n",
    "# Train sklearn's optimized Random Forest for performance comparison\n",
    "\n",
    "print(\"=\"*60)\n",
    "print(\"TRAINING SCIKIT-LEARN RANDOM FOREST FOR COMPARISON\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Import sklearn's Random Forest implementation\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# Create sklearn Random Forest with similar parameters to our custom implementation\n",
    "sklearn_rf = RandomForestClassifier(\n",
    "    n_estimators=100,       # Same number of trees as our custom RF\n",
    "    max_depth=10,          # Same maximum depth\n",
    "    random_state=42        # Same random seed for fair comparison\n",
    "    # Note: sklearn uses different default values for min_samples_split and max_features\n",
    ")\n",
    "\n",
    "# Train the sklearn model\n",
    "# Note: sklearn expects 1D array for y, so we use ravel() to flatten\n",
    "sklearn_rf.fit(X_train, y_train.ravel())\n",
    "\n",
    "# Make predictions using the trained sklearn model\n",
    "sklearn_predictions = sklearn_rf.predict(X_test)\n",
    "print(\"scikit-learn Random Forest Predictions Completed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "997dadc5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "==================================================\n",
      "SCIKIT-LEARN RANDOM FOREST PERFORMANCE METRICS\n",
      "==================================================\n",
      "Confusion Matrix:\n",
      "[[70  1]\n",
      " [ 3 40]]\n",
      "\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.99      0.97        71\n",
      "           1       0.98      0.93      0.95        43\n",
      "\n",
      "    accuracy                           0.96       114\n",
      "   macro avg       0.97      0.96      0.96       114\n",
      "weighted avg       0.97      0.96      0.96       114\n",
      "\n",
      "Accuracy: 96.49%\n"
     ]
    }
   ],
   "source": [
    "print(\"=\"*50)\n",
    "print(\"SCIKIT-LEARN RANDOM FOREST PERFORMANCE METRICS\")\n",
    "print(\"=\"*50)\n",
    "# Evaluate sklearn model's performance using the same metrics\n",
    "print(\"Confusion Matrix:\")\n",
    "print(confusion_matrix(y_test, sklearn_predictions))\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, sklearn_predictions))\n",
    "print(f\"Accuracy: {accuracy_score(y_test, sklearn_predictions)*100:.2f}%\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
